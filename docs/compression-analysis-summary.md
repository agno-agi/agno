# Context Compression Analysis Summary

## Overview

This document summarizes the analysis of the mid-run context compression feature in Agno, including verification of the compression algorithm, token counting accuracy, and identified issues.

---

## 1. Compression Algorithm Verification

### Algorithm Design

The compression algorithm follows these steps:

1. **Find current user**: `role == "user"` AND `from_history == False` (latest user input in current run)
2. **Find last tool batch**: Scan from end for `assistant` with `tool_calls` + corresponding `tool` messages
3. **Keep**: System messages + current user + last tool batch
4. **Compress**: Everything else (history messages + earlier tool batches) into a summary

### Message Structure After Compression

```
BEFORE COMPRESSION:
┌─────────────────────────────────────────────────────┐
│ [system]                                            │
│ [user] ─────────┐                                   │
│ [assistant]     │                                   │
│ [user]          │ HISTORY (from_history=True)       │
│ [assistant]     │ → COMPRESS INTO SUMMARY           │
│ [tool]          │                                   │
│ [assistant] ────┘                                   │
│ [user]          ← CURRENT USER (preserve)           │
│ [assistant]     ← LAST TOOL BATCH (preserve)        │
│ [tool]          ← LAST TOOL BATCH (preserve)        │
└─────────────────────────────────────────────────────┘

AFTER COMPRESSION:
┌─────────────────────────────────────────────────────┐
│ [system]                                            │
│ [user: summary]  ← Injected summary (role=user)     │
│ [user]           ← Current user message             │
│ [assistant]      ← Last tool batch (if exists)      │
│ [tool]           ← Last tool batch (if exists)      │
└─────────────────────────────────────────────────────┘
```

### Verification Results

| Aspect | Status | Evidence |
|--------|--------|----------|
| Finding current user | ✅ | `found current user at index 10/6` |
| Finding last tool batch | ✅ | `found last tool batch at index 11 (2 messages)` |
| Compressing history | ✅ | 9 msgs → summary, 5 msgs → summary |
| Preserving system | ✅ | `keeping 1 system messages` |
| Preserving current user | ✅ | Current user message preserved |
| Preserving tool batch | ✅ | `keeping 2 messages from last tool batch` |
| Merging tracked IDs | ✅ | `15 → 24 → 29` IDs accumulated across runs |
| Message filtering on load | ✅ | All compressed IDs filtered correctly |

---

## 2. Token Counting Accuracy

### Comparison: Our Estimation vs Google AI Studio Actual

We verified our token counting against actual API usage from Google AI Studio:

| Metric | Our Estimation | Actual API (Google AI Studio) | Difference |
|--------|---------------|-------------------------------|------------|
| Phase 2 prompt tokens | 20,679 | 21,527 | ~4% |
| Phase 3 prompt tokens | 2,596 | 2,878 | ~10% |

**Conclusion**: Our `count_tokens` estimation is highly accurate (within 4-10%).

### Token Types (Gemini)

```
prompt_token_count: 21,527    ← Input tokens (we CAN estimate)
candidates_token_count: 1,446 ← Output tokens (we CANNOT predict)
thoughts_token_count: 847     ← Thinking tokens (we CANNOT predict)
```

**Note**: We can only estimate **input tokens** before sending. Thinking and output tokens are generated by the model during inference.

---

## 3. Identified Bug: Streaming Metrics Accumulation

### The Problem

Agno incorrectly accumulates metrics from Gemini streaming chunks.

**Evidence from logs**:
- Each streaming chunk reports the same `prompt=21527` (cumulative total)
- Our code adds them: `21,527 × 58 chunks = 1,248,566`
- Actual API usage: `21,527` tokens

### Root Cause

In `libs/agno/agno/models/base.py` (lines 1714-1716):

```python
if stream_data.response_metrics is None:
    stream_data.response_metrics = Metrics()
stream_data.response_metrics += model_response_delta.response_usage  # BUG: accumulates
```

Gemini sends **cumulative totals** in each streaming chunk, but our code treats them as **incremental deltas**.

### Proposed Fix

For streaming responses, **replace** the metrics with the latest chunk's values instead of accumulating them, or only use the **final chunk's metrics**.

**Status**: Deferred for later fix.

---

## 4. Other Issues Found

### Model Hallucination

The model hallucinated a non-existent tool call:

```
Name: 'run'
Arguments: 'query: Anthropic competitors in AI/LLM space'
ERROR Function run not found
```

The correct tool should have been `duckduckgo_search`.

### Failed Tool Call Persisted in History

The hallucinated `run` tool call and its error result were saved into session history, polluting subsequent runs:

```
[6] role=assistant, tool_calls=['run']
[7] role=tool, tool_call_id=6e87d868-...
```

**Potential Enhancement**: Filter out failed tool calls from history to avoid context pollution.

---

## 5. Key Files Modified

| File | Changes |
|------|---------|
| `libs/agno/agno/compression/manager.py` | Refactored `_compress_context` algorithm, added verbose debug logs |
| `libs/agno/agno/session/agent.py` | Added `filter_compressed` parameter to `get_messages()` |
| `libs/agno/agno/session/team.py` | Same `filter_compressed` changes |
| `libs/agno/agno/agent/agent.py` | Updated to use `filter_compressed`, added debug logs |
| `libs/agno/agno/models/base.py` | Added post-compression token count logging |
| `libs/agno/agno/models/google/gemini.py` | Added detailed token breakdown logging |

---

## 6. Compression Effectiveness

### Token Reduction Results

| Phase | Before Compression | After Compression | Reduction |
|-------|-------------------|-------------------|-----------|
| Phase 2 | 26,682 tokens | 21,527 tokens | 19% |
| Phase 3 | 22,154 tokens | 2,878 tokens | **87%** |

### Character Reduction

| Phase | Before | After | Reduction |
|-------|--------|-------|-----------|
| Phase 2 | 35,458 chars | 5,508 chars | 84% |
| Phase 3 | 76,200 chars | 5,695 chars | 93% |

---

## 7. Summary

1. **Compression algorithm** correctly identifies current user, last tool batch, and compresses history
2. **Token estimation** is accurate (within 4-10% of actual API usage)
3. **Session persistence** correctly saves/loads compressed context IDs
4. **Message filtering** correctly excludes compressed messages on subsequent runs
5. **Summary merging** accumulates tracked IDs across multiple compressions


---

## 8. Test Commands

```bash
# Run the compression cookbook example
source /Users/coolm/Developer/agno/.venv/bin/activate
uv run cookbook/agents/context_compression/competitor_analysis_compression.py

# Run unit tests
python -m pytest libs/agno/tests/unit/compression/test_context_compression.py -v

# Run integration tests
python -m pytest libs/agno/tests/integration/agent/test_context_compression.py -v
```

---

*Generated: December 13, 2025*
